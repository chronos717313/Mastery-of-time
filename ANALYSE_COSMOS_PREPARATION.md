# Analyse COSMOS : PrÃ©paration et ImplÃ©mentation
## Test CorrÃ©lation Î¸_halo â†” Î¸_voisin

**Date** : 2025-12-06
**Auteur** : Pierre-Olivier DesprÃ©s Asselin
**Objectif** : Tester prÃ©diction MT avec donnÃ©es publiques COSMOS

---

## 1. Vue d'Ensemble COSMOS

### COSMOS Survey (Cosmic Evolution Survey)

**Site officiel** : https://cosmos.astro.caltech.edu/

**CaractÃ©ristiques** :
- **Champ** : 2 degÂ² (rÃ©gion Ã©quatoriale)
- **Profondeur** : Observations HST + Subaru + VLT
- **Galaxies** : ~2 millions de galaxies photomÃ©triques
- **Weak lensing** : ~500,000 galaxies avec mesures shear
- **Redshift** : 0.2 < z < 1.2 (photomÃ©trique)
- **Catalogues publics** : OUI âœ…

**Avantages pour notre test** :
- âœ… DonnÃ©es weak lensing de haute qualitÃ©
- âœ… Catalogues galaxies voisines disponibles
- âœ… Redshifts photomÃ©triques prÃ©cis
- âœ… Masses stellaires estimÃ©es
- âœ… TÃ©lÃ©chargement gratuit

---

## 2. DonnÃ©es NÃ©cessaires

### A) Catalogue Weak Lensing COSMOS

**Fichier** : `COSMOS_shear_catalog.fits`

**Contenu requis** :
- `RA`, `DEC` : Position (degrÃ©s)
- `z_photo` : Redshift photomÃ©trique
- `e1`, `e2` : Composantes ellipticitÃ© (shear)
- `weight` : Poids statistique
- `theta_halo` : Position angle du halo (calculÃ© depuis e1, e2)

**Source** : COSMOS2015 catalog
**Lien** : https://cosmos.astro.caltech.edu/page/data-releases

### B) Catalogue Galaxies Massives

**Fichier** : `COSMOS_galaxies_massives.fits`

**Contenu requis** :
- `RA`, `DEC` : Position
- `z_photo` : Redshift
- `M_stellar` : Masse stellaire (Mâ˜‰)
- `M_halo` : Masse du halo (estimÃ©e, si disponible)

**CritÃ¨re sÃ©lection** : M_stellar > 10Â¹Â¹ Mâ˜‰ (voisins massifs)

### C) Catalogue Paires Galaxie-Voisin

**Ã€ crÃ©er** : `COSMOS_paires_analyse.fits`

**Contenu** :
- ID galaxie lentille (avec weak lensing)
- Î¸_halo : Orientation du halo (0-180Â°)
- e_halo : EllipticitÃ© du halo
- ID voisin le plus massif
- Î¸_voisin : Direction vers voisin (0-360Â°)
- M_voisin : Masse du voisin
- d_voisin : Distance projetÃ©e (Mpc)
- Î”z : DiffÃ©rence redshift

---

## 3. TÃ©lÃ©chargement des DonnÃ©es

### Script de TÃ©lÃ©chargement

```bash
#!/bin/bash
# download_cosmos_data.sh

# CrÃ©er rÃ©pertoire donnÃ©es
mkdir -p data/COSMOS
cd data/COSMOS

# COSMOS2015 Catalog (galaxies)
echo "TÃ©lÃ©chargement COSMOS2015 catalog..."
wget https://cosmos.astro.caltech.edu/data/COSMOS2015_Laigle+_v1.1.fits
mv COSMOS2015_Laigle+_v1.1.fits cosmos2015_galaxies.fits

# COSMOS Shear Catalog (weak lensing)
echo "TÃ©lÃ©chargement COSMOS shear catalog..."
wget https://cosmos.astro.caltech.edu/data/shear/cosmos_shear_v2.fits
mv cosmos_shear_v2.fits cosmos_shear.fits

# VÃ©rification
echo "Fichiers tÃ©lÃ©chargÃ©s:"
ls -lh

echo "âœ“ TÃ©lÃ©chargement terminÃ©"
```

**ExÃ©cution** :
```bash
chmod +x download_cosmos_data.sh
./download_cosmos_data.sh
```

**Taille totale** : ~2-3 GB

---

## 4. Code Python : PrÃ©paration des DonnÃ©es

### Script 1 : Lecture et Nettoyage

```python
#!/usr/bin/env python3
# 01_prepare_cosmos_data.py
"""
PrÃ©paration donnÃ©es COSMOS pour analyse Î¸_halo â†” Î¸_voisin
"""

import numpy as np
import pandas as pd
from astropy.io import fits
from astropy.coordinates import SkyCoord
from astropy import units as u
from astropy.cosmology import FlatLambdaCDM
import matplotlib.pyplot as plt

# Cosmologie
cosmo = FlatLambdaCDM(H0=70, Om0=0.3)

# ============================================
# 1. CHARGER DONNÃ‰ES COSMOS
# ============================================

def load_cosmos_shear(filename='data/COSMOS/cosmos_shear.fits'):
    """
    Charge catalogue weak lensing COSMOS
    """
    print("Chargement catalogue shear COSMOS...")

    with fits.open(filename) as hdul:
        data = hdul[1].data

    # CrÃ©er DataFrame
    df = pd.DataFrame({
        'ID': data['NUMBER'],
        'RA': data['ALPHA_J2000'],
        'DEC': data['DELTA_J2000'],
        'z_photo': data['PHOTOZ'],
        'e1': data['E1'],
        'e2': data['E2'],
        'weight': data['WEIGHT'],
        'mag_i': data['MAG_AUTO']  # Magnitude bande i
    })

    print(f"  â†’ {len(df)} galaxies chargÃ©es")

    return df

def load_cosmos_galaxies(filename='data/COSMOS/cosmos2015_galaxies.fits'):
    """
    Charge catalogue galaxies COSMOS2015
    """
    print("Chargement catalogue galaxies COSMOS2015...")

    with fits.open(filename) as hdul:
        data = hdul[1].data

    # CrÃ©er DataFrame
    df = pd.DataFrame({
        'ID': data['NUMBER'],
        'RA': data['ALPHA_J2000'],
        'DEC': data['DELTA_J2000'],
        'z_photo': data['PHOTOZ'],
        'z_spec': data['ZQ'],  # Redshift spectroscopique si disponible
        'M_stellar': 10**data['MASS_MED'],  # Masse stellaire en Mâ˜‰
        'mag_i': data['ip_MAG_AUTO']
    })

    print(f"  â†’ {len(df)} galaxies chargÃ©es")

    return df

# ============================================
# 2. CALCULER ORIENTATION HALOS
# ============================================

def calculate_halo_orientation(e1, e2):
    """
    Calcule angle de position du halo depuis ellipticitÃ©

    Î¸_halo = 0.5 * arctan2(e2, e1)  [radians]

    Convention: 0Â° = Nord, 90Â° = Est
    Retourne angles dans [0, 180Â°] (ellipse symÃ©trique)
    """
    theta_rad = 0.5 * np.arctan2(e2, e1)
    theta_deg = np.degrees(theta_rad)

    # Ramener Ã  [0, 180Â°]
    theta_deg = theta_deg % 180

    return theta_deg

def calculate_ellipticity_modulus(e1, e2):
    """
    Calcule module ellipticitÃ© e = âˆš(e1Â² + e2Â²)
    """
    return np.sqrt(e1**2 + e2**2)

# ============================================
# 3. SÃ‰LECTION Ã‰CHANTILLON
# ============================================

def select_lensing_sample(df_shear,
                          z_min=0.2, z_max=0.6,
                          e_min=0.1, e_max=0.8,
                          weight_min=0.5):
    """
    SÃ©lectionne galaxies avec weak lensing fiable
    """
    print("\nSÃ©lection Ã©chantillon weak lensing...")
    print(f"  CritÃ¨res:")
    print(f"    {z_min} < z < {z_max}")
    print(f"    {e_min} < e < {e_max}")
    print(f"    weight > {weight_min}")

    # Calculer ellipticitÃ©
    df_shear['e_modulus'] = calculate_ellipticity_modulus(
        df_shear['e1'], df_shear['e2']
    )

    # Calculer orientation
    df_shear['theta_halo'] = calculate_halo_orientation(
        df_shear['e1'], df_shear['e2']
    )

    # SÃ©lection
    mask = (
        (df_shear['z_photo'] >= z_min) &
        (df_shear['z_photo'] <= z_max) &
        (df_shear['e_modulus'] >= e_min) &
        (df_shear['e_modulus'] <= e_max) &
        (df_shear['weight'] >= weight_min)
    )

    df_selected = df_shear[mask].copy()

    print(f"  â†’ {len(df_selected)} galaxies sÃ©lectionnÃ©es")
    print(f"     ({100*len(df_selected)/len(df_shear):.1f}% de l'Ã©chantillon)")

    return df_selected

def select_massive_neighbors(df_galaxies, M_min=1e11):
    """
    SÃ©lectionne galaxies massives (voisins potentiels)
    """
    print(f"\nSÃ©lection voisins massifs (M > {M_min:.0e} Mâ˜‰)...")

    mask = df_galaxies['M_stellar'] >= M_min
    df_massive = df_galaxies[mask].copy()

    print(f"  â†’ {len(df_massive)} galaxies massives")

    return df_massive

# ============================================
# 4. TROUVER VOISINS
# ============================================

def find_nearest_massive_neighbor(lens_coords, lens_z,
                                  neighbor_coords, neighbor_z, neighbor_mass,
                                  d_max_physical=2.0,  # Mpc
                                  dz_max=0.05):
    """
    Trouve voisin le plus massif pour chaque galaxie lentille

    Parameters:
    - lens_coords: SkyCoord des lentilles
    - neighbor_coords: SkyCoord des voisins
    - d_max_physical: Distance physique maximale (Mpc)
    - dz_max: DiffÃ©rence redshift maximale

    Returns:
    - idx_neighbors: Indices voisins (ou -1 si pas de voisin)
    - separations: SÃ©parations angulaires (deg)
    - position_angles: Angles de position (deg, 0=N, 90=E)
    """
    n_lens = len(lens_coords)
    idx_neighbors = np.full(n_lens, -1, dtype=int)
    separations_angular = np.full(n_lens, np.nan)
    position_angles = np.full(n_lens, np.nan)
    separations_physical = np.full(n_lens, np.nan)

    print(f"\nRecherche voisins pour {n_lens} galaxies...")

    for i in range(n_lens):
        if i % 1000 == 0:
            print(f"  Progression: {i}/{n_lens} ({100*i/n_lens:.1f}%)")

        # CoordonnÃ©es lentille
        lens_coord = lens_coords[i:i+1]
        z_lens = lens_z.iloc[i]

        # Contrainte redshift
        dz = np.abs(neighbor_z - z_lens)
        mask_z = dz <= dz_max

        if mask_z.sum() == 0:
            continue

        # SÃ©parations angulaires
        seps = lens_coord.separation(neighbor_coords[mask_z])
        seps_deg = seps.deg

        # Conversion en distance physique projetÃ©e
        D_A = cosmo.angular_diameter_distance(z_lens).value  # Mpc
        seps_physical = D_A * np.radians(seps_deg)  # Mpc

        # Contrainte distance physique
        mask_dist = seps_physical <= d_max_physical

        if mask_dist.sum() == 0:
            continue

        # Indices voisins candidats (dans neighbor_coords)
        idx_candidates_local = np.where(mask_z)[0][mask_dist]

        # Trouver le plus massif
        masses_candidates = neighbor_mass.iloc[idx_candidates_local]
        idx_most_massive_local = masses_candidates.idxmax()
        idx_most_massive_global = idx_candidates_local[
            masses_candidates.index.get_loc(idx_most_massive_local)
        ]

        # Stocker rÃ©sultats
        idx_neighbors[i] = idx_most_massive_global
        separations_angular[i] = seps_deg[mask_dist][
            masses_candidates.index.get_loc(idx_most_massive_local)
        ]
        separations_physical[i] = seps_physical[mask_dist][
            masses_candidates.index.get_loc(idx_most_massive_local)
        ]

        # Position angle (direction lens â†’ voisin)
        pa = lens_coord.position_angle(
            neighbor_coords[idx_most_massive_global:idx_most_massive_global+1]
        )
        position_angles[i] = pa.deg

    n_found = (idx_neighbors >= 0).sum()
    print(f"  â†’ {n_found} voisins trouvÃ©s ({100*n_found/n_lens:.1f}%)")

    return idx_neighbors, separations_angular, position_angles, separations_physical

# ============================================
# 5. CRÃ‰ER CATALOGUE PAIRES
# ============================================

def create_pairs_catalog(df_lensing, df_massive):
    """
    CrÃ©e catalogue de paires galaxie-voisin
    """
    print("\n" + "="*60)
    print("CRÃ‰ATION CATALOGUE PAIRES")
    print("="*60)

    # Convertir en SkyCoord
    lens_coords = SkyCoord(
        ra=df_lensing['RA'].values*u.deg,
        dec=df_lensing['DEC'].values*u.deg
    )

    neighbor_coords = SkyCoord(
        ra=df_massive['RA'].values*u.deg,
        dec=df_massive['DEC'].values*u.deg
    )

    # Trouver voisins
    idx_neighbors, sep_ang, pos_angles, sep_phys = find_nearest_massive_neighbor(
        lens_coords,
        df_lensing['z_photo'],
        neighbor_coords,
        df_massive['z_photo'],
        df_massive['M_stellar'],
        d_max_physical=2.0,
        dz_max=0.05
    )

    # CrÃ©er DataFrame paires
    mask_valid = idx_neighbors >= 0

    df_pairs = pd.DataFrame({
        'lens_ID': df_lensing.iloc[mask_valid]['ID'].values,
        'lens_RA': df_lensing.iloc[mask_valid]['RA'].values,
        'lens_DEC': df_lensing.iloc[mask_valid]['DEC'].values,
        'lens_z': df_lensing.iloc[mask_valid]['z_photo'].values,
        'theta_halo': df_lensing.iloc[mask_valid]['theta_halo'].values,
        'e_halo': df_lensing.iloc[mask_valid]['e_modulus'].values,

        'neighbor_ID': df_massive.iloc[idx_neighbors[mask_valid]]['ID'].values,
        'neighbor_M': df_massive.iloc[idx_neighbors[mask_valid]]['M_stellar'].values,
        'neighbor_z': df_massive.iloc[idx_neighbors[mask_valid]]['z_photo'].values,
        'theta_neighbor': pos_angles[mask_valid],
        'separation_ang': sep_ang[mask_valid],
        'separation_Mpc': sep_phys[mask_valid],
    })

    print(f"\nCatalogue paires crÃ©Ã©: {len(df_pairs)} paires")

    return df_pairs

# ============================================
# 6. MAIN
# ============================================

if __name__ == "__main__":

    print("="*60)
    print("PRÃ‰PARATION DONNÃ‰ES COSMOS")
    print("Test Î¸_halo â†” Î¸_voisin")
    print("="*60)

    # 1. Charger donnÃ©es
    df_shear = load_cosmos_shear()
    df_galaxies = load_cosmos_galaxies()

    # 2. SÃ©lectionner Ã©chantillons
    df_lensing = select_lensing_sample(df_shear)
    df_massive = select_massive_neighbors(df_galaxies, M_min=1e11)

    # 3. CrÃ©er catalogue paires
    df_pairs = create_pairs_catalog(df_lensing, df_massive)

    # 4. Sauvegarder
    output_file = 'data/COSMOS/cosmos_pairs_analysis.csv'
    df_pairs.to_csv(output_file, index=False)
    print(f"\nâœ“ Catalogue sauvegardÃ©: {output_file}")

    # 5. Statistiques
    print("\n" + "="*60)
    print("STATISTIQUES Ã‰CHANTILLON")
    print("="*60)
    print(f"Nombre de paires: {len(df_pairs)}")
    print(f"Redshift moyen lentilles: {df_pairs['lens_z'].mean():.3f}")
    print(f"Masse moyenne voisins: {df_pairs['neighbor_M'].mean():.2e} Mâ˜‰")
    print(f"SÃ©paration moyenne: {df_pairs['separation_Mpc'].mean():.2f} Mpc")
    print(f"EllipticitÃ© moyenne halos: {df_pairs['e_halo'].mean():.3f}")

    print("\nâœ“ PrÃ©paration terminÃ©e!")
```

---

## 5. Code Python : Analyse CorrÃ©lation

### Script 2 : Test Î¸_halo â†” Î¸_voisin

```python
#!/usr/bin/env python3
# 02_analyze_correlation.py
"""
Analyse corrÃ©lation Î¸_halo â†” Î¸_voisin
Test dÃ©cisif MaÃ®trise du Temps vs Lambda-CDM
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import pearsonr, spearmanr
from scipy.stats import vonmises
import seaborn as sns

# ============================================
# 1. CHARGER DONNÃ‰ES
# ============================================

def load_pairs(filename='data/COSMOS/cosmos_pairs_analysis.csv'):
    """
    Charge catalogue paires
    """
    print("Chargement catalogue paires...")
    df = pd.read_csv(filename)
    print(f"  â†’ {len(df)} paires chargÃ©es")
    return df

# ============================================
# 2. CALCULER DIFFÃ‰RENCE ANGULAIRE
# ============================================

def calculate_angular_difference(theta_halo, theta_neighbor):
    """
    Calcule diffÃ©rence angulaire Î”Î¸ = Î¸_neighbor - Î¸_halo

    RamÃ¨ne Ã  [-90Â°, 90Â°] car ellipse symÃ©trique (Î¸ et Î¸+180Â° Ã©quivalents)
    """
    # theta_halo dans [0, 180Â°]
    # theta_neighbor dans [0, 360Â°]

    # Convertir theta_neighbor dans [0, 180Â°] aussi
    theta_neighbor_mod = theta_neighbor % 180

    # DiffÃ©rence
    delta = theta_neighbor_mod - theta_halo

    # Ramener Ã  [-90, 90]
    delta[delta > 90] -= 180
    delta[delta < -90] += 180

    return delta

# ============================================
# 3. TESTS STATISTIQUES
# ============================================

def test_correlation(df_pairs):
    """
    Tests de corrÃ©lation Î¸_halo â†” Î¸_voisin
    """
    print("\n" + "="*60)
    print("TESTS DE CORRÃ‰LATION")
    print("="*60)

    # Calculer Î”Î¸
    df_pairs['delta_theta'] = calculate_angular_difference(
        df_pairs['theta_halo'].values,
        df_pairs['theta_neighbor'].values
    )

    # Test 1: CorrÃ©lation de Pearson
    print("\n1. CorrÃ©lation de Pearson (Î¸_halo vs Î¸_neighbor)")

    # Convertir theta_neighbor en [0, 180Â°] pour corrÃ©lation
    theta_neighbor_mod = df_pairs['theta_neighbor'].values % 180

    r_pearson, p_pearson = pearsonr(
        df_pairs['theta_halo'].values,
        theta_neighbor_mod
    )

    print(f"   r = {r_pearson:.4f}")
    print(f"   p-value = {p_pearson:.2e}")

    # InterprÃ©tation
    if r_pearson > 0.5:
        print("   â†’ FORTE CORRÃ‰LATION (MaÃ®trise du Temps favorisÃ©e)")
    elif r_pearson > 0.2:
        print("   â†’ CorrÃ©lation modÃ©rÃ©e (rÃ©sultat ambigu)")
    else:
        print("   â†’ Pas de corrÃ©lation (Lambda-CDM favorisÃ©)")

    # Test 2: Distribution Î”Î¸
    print("\n2. Distribution Î”Î¸ (alignement parfait = 0Â°)")

    mean_abs_delta = np.mean(np.abs(df_pairs['delta_theta']))
    std_delta = np.std(df_pairs['delta_theta'])

    print(f"   |Î”Î¸| moyen = {mean_abs_delta:.1f}Â°")
    print(f"   Ã‰cart-type = {std_delta:.1f}Â°")
    print(f"   Attendu si alÃ©atoire: |Î”Î¸| â‰ˆ 45Â°, Ïƒ â‰ˆ 26Â°")

    if mean_abs_delta < 30:
        print("   â†’ ALIGNEMENT SIGNIFICATIF (MaÃ®trise du Temps)")
    elif mean_abs_delta < 40:
        print("   â†’ Alignement partiel (ambigu)")
    else:
        print("   â†’ Pas d'alignement (Lambda-CDM)")

    # Test 3: Fraction alignÃ©e (|Î”Î¸| < 30Â°)
    print("\n3. Fraction galaxies bien alignÃ©es (|Î”Î¸| < 30Â°)")

    frac_aligned = (np.abs(df_pairs['delta_theta']) < 30).sum() / len(df_pairs)

    print(f"   Fraction = {100*frac_aligned:.1f}%")
    print(f"   Attendu si alÃ©atoire: 33%")
    print(f"   Attendu si MT (forte corrÃ©lation): > 60%")

    if frac_aligned > 0.6:
        print("   â†’ FORTE Ã‰VIDENCE MaÃ®trise du Temps")
    elif frac_aligned > 0.45:
        print("   â†’ Ã‰vidence modÃ©rÃ©e MT")
    else:
        print("   â†’ Pas d'Ã©vidence MT")

    # Test 4: Bootstrap pour intervalle de confiance
    print("\n4. Bootstrap (10000 itÃ©rations) pour IC 95% sur r")

    n_bootstrap = 10000
    r_bootstrap = []

    for _ in range(n_bootstrap):
        sample = df_pairs.sample(n=len(df_pairs), replace=True)
        theta_n_mod = sample['theta_neighbor'].values % 180
        r, _ = pearsonr(sample['theta_halo'].values, theta_n_mod)
        r_bootstrap.append(r)

    r_bootstrap = np.array(r_bootstrap)
    ci_low, ci_high = np.percentile(r_bootstrap, [2.5, 97.5])

    print(f"   r = {r_pearson:.4f}")
    print(f"   IC 95%: [{ci_low:.4f}, {ci_high:.4f}]")

    # RÃ©sultat global
    print("\n" + "="*60)
    print("RÃ‰SULTAT GLOBAL")
    print("="*60)

    score_MT = 0

    if r_pearson > 0.5: score_MT += 3
    elif r_pearson > 0.2: score_MT += 1

    if mean_abs_delta < 30: score_MT += 3
    elif mean_abs_delta < 40: score_MT += 1

    if frac_aligned > 0.6: score_MT += 3
    elif frac_aligned > 0.45: score_MT += 1

    print(f"Score MaÃ®trise du Temps: {score_MT}/9")

    if score_MT >= 7:
        print("VERDICT: FORTE Ã‰VIDENCE POUR MAÃŽTRISE DU TEMPS âœ“âœ“âœ“")
    elif score_MT >= 4:
        print("VERDICT: Ã‰vidence modÃ©rÃ©e pour MT, nÃ©cessite plus de donnÃ©es")
    else:
        print("VERDICT: Pas d'Ã©vidence pour MT, Lambda-CDM favorisÃ©")

    return r_pearson, p_pearson, df_pairs

# ============================================
# 4. ANALYSES SECONDAIRES
# ============================================

def analyze_mass_dependence(df_pairs):
    """
    CorrÃ©lation dÃ©pend-elle de la masse du voisin ?
    """
    print("\n" + "="*60)
    print("DÃ‰PENDANCE EN MASSE DU VOISIN")
    print("="*60)

    # Bins de masse
    mass_bins = [1e11, 3e11, 10e11, 1e13]
    mass_labels = ['10Â¹Â¹-3Ã—10Â¹Â¹', '3Ã—10Â¹Â¹-10Â¹Â²', '>10Â¹Â²']

    for i, (m_low, m_high) in enumerate(zip(mass_bins[:-1], mass_bins[1:])):
        mask = (
            (df_pairs['neighbor_M'] >= m_low) &
            (df_pairs['neighbor_M'] < m_high)
        )

        if mask.sum() < 10:
            continue

        subset = df_pairs[mask]
        theta_n_mod = subset['theta_neighbor'].values % 180
        r, p = pearsonr(subset['theta_halo'].values, theta_n_mod)
        mean_delta = np.mean(np.abs(subset['delta_theta']))

        print(f"\nMasse voisin: {mass_labels[i]} Mâ˜‰")
        print(f"  N = {len(subset)}")
        print(f"  r = {r:.3f} (p = {p:.2e})")
        print(f"  |Î”Î¸| = {mean_delta:.1f}Â°")

    print("\nPrÃ©diction MT: CorrÃ©lation augmente avec masse voisin")

def analyze_distance_dependence(df_pairs):
    """
    CorrÃ©lation dÃ©pend-elle de la distance au voisin ?
    """
    print("\n" + "="*60)
    print("DÃ‰PENDANCE EN DISTANCE")
    print("="*60)

    # Bins de distance
    dist_bins = [0.3, 0.8, 1.5, 2.0]
    dist_labels = ['0.3-0.8 Mpc', '0.8-1.5 Mpc', '1.5-2.0 Mpc']

    for i, (d_low, d_high) in enumerate(zip(dist_bins[:-1], dist_bins[1:])):
        mask = (
            (df_pairs['separation_Mpc'] >= d_low) &
            (df_pairs['separation_Mpc'] < d_high)
        )

        if mask.sum() < 10:
            continue

        subset = df_pairs[mask]
        theta_n_mod = subset['theta_neighbor'].values % 180
        r, p = pearsonr(subset['theta_halo'].values, theta_n_mod)
        mean_delta = np.mean(np.abs(subset['delta_theta']))

        print(f"\nDistance: {dist_labels[i]}")
        print(f"  N = {len(subset)}")
        print(f"  r = {r:.3f} (p = {p:.2e})")
        print(f"  |Î”Î¸| = {mean_delta:.1f}Â°")

    print("\nPrÃ©diction MT: CorrÃ©lation dÃ©croÃ®t avec distance")

# ============================================
# 5. VISUALISATIONS
# ============================================

def plot_results(df_pairs, r_pearson):
    """
    Graphiques des rÃ©sultats
    """
    print("\nCrÃ©ation graphiques...")

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))

    # 1. Histogramme Î”Î¸
    ax = axes[0, 0]
    ax.hist(df_pairs['delta_theta'], bins=36, range=(-90, 90),
            alpha=0.7, edgecolor='black', color='steelblue')
    ax.axvline(0, color='red', linestyle='--', linewidth=2,
               label='Alignement parfait')
    ax.set_xlabel('Î”Î¸ = Î¸_neighbor - Î¸_halo (Â°)', fontsize=12)
    ax.set_ylabel('Nombre de galaxies', fontsize=12)
    ax.set_title('Distribution des diffÃ©rences angulaires', fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)

    # Ajouter texte statistique
    mean_delta = np.mean(np.abs(df_pairs['delta_theta']))
    ax.text(0.05, 0.95, f'|Î”Î¸| moyen = {mean_delta:.1f}Â°\nAttendu alÃ©atoire: 45Â°',
            transform=ax.transAxes, fontsize=11, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    # 2. Scatter Î¸_halo vs Î¸_neighbor
    ax = axes[0, 1]
    theta_n_mod = df_pairs['theta_neighbor'].values % 180
    ax.scatter(df_pairs['theta_halo'], theta_n_mod, alpha=0.3, s=10)
    ax.plot([0, 180], [0, 180], 'r--', linewidth=2, label='Alignement parfait (y=x)')
    ax.set_xlabel('Î¸_halo (Â°)', fontsize=12)
    ax.set_ylabel('Î¸_neighbor (Â°)', fontsize=12)
    ax.set_title(f'CorrÃ©lation Î¸_halo â†” Î¸_neighbor\nr = {r_pearson:.3f}', fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    ax.set_xlim(0, 180)
    ax.set_ylim(0, 180)

    # 3. |Î”Î¸| vs Masse voisin
    ax = axes[0, 2]
    ax.scatter(df_pairs['neighbor_M'], np.abs(df_pairs['delta_theta']),
               alpha=0.3, s=10)
    ax.axhline(45, color='red', linestyle='--', label='Attendu si alÃ©atoire')
    ax.set_xlabel('Masse voisin (Mâ˜‰)', fontsize=12)
    ax.set_ylabel('|Î”Î¸| (Â°)', fontsize=12)
    ax.set_title('Alignement vs Masse du voisin', fontsize=14, fontweight='bold')
    ax.set_xscale('log')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)

    # 4. |Î”Î¸| vs Distance
    ax = axes[1, 0]
    ax.scatter(df_pairs['separation_Mpc'], np.abs(df_pairs['delta_theta']),
               alpha=0.3, s=10)
    ax.axhline(45, color='red', linestyle='--', label='Attendu si alÃ©atoire')
    ax.set_xlabel('Distance au voisin (Mpc)', fontsize=12)
    ax.set_ylabel('|Î”Î¸| (Â°)', fontsize=12)
    ax.set_title('Alignement vs Distance', fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)

    # 5. Fraction alignÃ©e par bin de masse
    ax = axes[1, 1]
    mass_bins = np.logspace(11, 12.5, 8)
    frac_aligned_bins = []
    mass_centers = []

    for i in range(len(mass_bins)-1):
        mask = (
            (df_pairs['neighbor_M'] >= mass_bins[i]) &
            (df_pairs['neighbor_M'] < mass_bins[i+1])
        )
        if mask.sum() > 5:
            frac = (np.abs(df_pairs.loc[mask, 'delta_theta']) < 30).sum() / mask.sum()
            frac_aligned_bins.append(frac)
            mass_centers.append(np.sqrt(mass_bins[i] * mass_bins[i+1]))

    ax.plot(mass_centers, frac_aligned_bins, 'o-', linewidth=2, markersize=8)
    ax.axhline(0.33, color='red', linestyle='--', label='Attendu LCDM (33%)')
    ax.axhline(0.60, color='green', linestyle='--', label='Attendu MT (>60%)')
    ax.set_xlabel('Masse voisin (Mâ˜‰)', fontsize=12)
    ax.set_ylabel('Fraction bien alignÃ©e (|Î”Î¸|<30Â°)', fontsize=12)
    ax.set_title('Fraction alignÃ©e vs Masse', fontsize=14, fontweight='bold')
    ax.set_xscale('log')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)

    # 6. Distribution ellipticitÃ©
    ax = axes[1, 2]
    ax.hist(df_pairs['e_halo'], bins=30, alpha=0.7, edgecolor='black', color='coral')
    ax.axvline(df_pairs['e_halo'].mean(), color='red', linestyle='--',
               linewidth=2, label=f'Moyenne = {df_pairs["e_halo"].mean():.3f}')
    ax.set_xlabel('EllipticitÃ© halo e', fontsize=12)
    ax.set_ylabel('Nombre de galaxies', fontsize=12)
    ax.set_title('Distribution ellipticitÃ©s', fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('results/cosmos_correlation_analysis.png', dpi=300, bbox_inches='tight')
    print("  âœ“ Graphique sauvegardÃ©: results/cosmos_correlation_analysis.png")

    plt.show()

# ============================================
# 6. MAIN
# ============================================

if __name__ == "__main__":

    print("="*60)
    print("ANALYSE CORRÃ‰LATION Î¸_halo â†” Î¸_voisin")
    print("Test MaÃ®trise du Temps vs Lambda-CDM")
    print("="*60)

    # CrÃ©er rÃ©pertoire rÃ©sultats
    import os
    os.makedirs('results', exist_ok=True)

    # 1. Charger donnÃ©es
    df_pairs = load_pairs()

    # 2. Test corrÃ©lation principal
    r_pearson, p_pearson, df_pairs = test_correlation(df_pairs)

    # 3. Analyses secondaires
    analyze_mass_dependence(df_pairs)
    analyze_distance_dependence(df_pairs)

    # 4. Visualisations
    plot_results(df_pairs, r_pearson)

    # 5. Sauvegarder rÃ©sultats dÃ©taillÃ©s
    df_pairs.to_csv('results/cosmos_pairs_with_analysis.csv', index=False)
    print("\nâœ“ RÃ©sultats sauvegardÃ©s: results/cosmos_pairs_with_analysis.csv")

    print("\n" + "="*60)
    print("ANALYSE TERMINÃ‰E")
    print("="*60)
```

---

## 6. Instructions d'ExÃ©cution

### Ã‰tape par Ã‰tape

```bash
# 1. TÃ©lÃ©charger donnÃ©es COSMOS
bash download_cosmos_data.sh

# 2. PrÃ©parer donnÃ©es (crÃ©ation catalogue paires)
python3 01_prepare_cosmos_data.py

# 3. Analyse corrÃ©lation
python3 02_analyze_correlation.py

# 4. RÃ©sultats dans:
#    - results/cosmos_correlation_analysis.png
#    - results/cosmos_pairs_with_analysis.csv
```

### Temps EstimÃ©

- TÃ©lÃ©chargement: 30-60 min (dÃ©pend connexion)
- PrÃ©paration: 10-20 min (recherche voisins)
- Analyse: 2-5 min
- **Total: ~1-2 heures**

---

## 7. RÃ©sultats Attendus

### ScÃ©nario A: MaÃ®trise du Temps ValidÃ©e

**Si r > 0.5, |Î”Î¸| < 30Â°, frac_aligned > 60%** :

```
RÃ‰SULTAT GLOBAL
Score MaÃ®trise du Temps: 9/9
VERDICT: FORTE Ã‰VIDENCE POUR MAÃŽTRISE DU TEMPS âœ“âœ“âœ“

CorrÃ©lation de Pearson: r = 0.67 (p < 10â»Â²â°)
|Î”Î¸| moyen = 24.3Â° (attendu alÃ©atoire: 45Â°)
Fraction bien alignÃ©e = 68% (attendu: 33%)
```

**Implication** : Publication immÃ©diate ApJ Letters + soumission Nature Astronomy

---

### ScÃ©nario B: Lambda-CDM ConfirmÃ©

**Si r < 0.2, |Î”Î¸| â‰ˆ 45Â°, frac_aligned â‰ˆ 33%** :

```
RÃ‰SULTAT GLOBAL
Score MaÃ®trise du Temps: 0/9
VERDICT: Pas d'Ã©vidence pour MT, Lambda-CDM favorisÃ©

CorrÃ©lation de Pearson: r = 0.08 (p = 0.42)
|Î”Î¸| moyen = 44.7Â° (cohÃ©rent avec alÃ©atoire)
Fraction bien alignÃ©e = 34% (comme attendu)
```

**Implication** : MT rÃ©futÃ©e, mais rÃ©sultat publiable (exclusion thÃ©orie)

---

### ScÃ©nario C: RÃ©sultat Ambigu

**Si 0.2 < r < 0.5** :

```
RÃ‰SULTAT GLOBAL
Score MaÃ®trise du Temps: 4/9
VERDICT: Ã‰vidence modÃ©rÃ©e pour MT, nÃ©cessite plus de donnÃ©es

CorrÃ©lation de Pearson: r = 0.34 (p = 10â»â¸)
|Î”Î¸| moyen = 36.2Â°
Fraction bien alignÃ©e = 47%
```

**Implication** : Signal dÃ©tectÃ© mais pas dÃ©cisif, analyse UNIONS cruciale

---

## 8. Prochaines Ã‰tapes Selon RÃ©sultats

### Si COSMOS Positif (r > 0.5)

1. **ImmÃ©diat** : RÃ©diger article court (ApJ Letters, 4 pages)
2. **Semaine 1** : Soumettre preprint arXiv
3. **Semaine 2-4** : Analyse UNIONS pour confirmation indÃ©pendante
4. **Mois 2** : Soumission journal (ApJ Letters ou Nature Astronomy)

### Si COSMOS Ambigu (0.2 < r < 0.5)

1. **Attendre** rÃ©ponse UNIONS (donnÃ©es plus prÃ©cises)
2. **Combiner** COSMOS + UNIONS pour statistiques accrues
3. **Analyser** sous-Ã©chantillons (haute masse, courte distance)

### Si COSMOS NÃ©gatif (r < 0.2)

1. **VÃ©rifier** code et procÃ©dure (erreurs possibles ?)
2. **Tester** avec Ã©chantillon UNIONS plus prÃ©cis
3. **Si confirmÃ©** : MT rÃ©futÃ©e, publier exclusion

---

## 9. Points d'Attention

### Biais Possibles

âš ï¸ **Biais de sÃ©lection** : Galaxies avec weak lensing dÃ©tectable peuvent Ãªtre biaisÃ©es

âš ï¸ **Erreurs redshift photomÃ©trique** : Peuvent crÃ©er fausses paires

âš ï¸ **Projection 3D â†’ 2D** : Voisin physiquement proche peut sembler Ã©loignÃ©

**Mitigation** : Contraintes strictes Î”z, distance physique, poids statistiques

---

## 10. RÃ©sumÃ©

**Cette analyse COSMOS vous permet de** :

âœ… Tester prÃ©diction MT **immÃ©diatement** (1-2 heures calcul)

âœ… Obtenir rÃ©sultat **avant** rÃ©ponse UNIONS

âœ… Calibrer mÃ©thode pour analyse UNIONS ultÃ©rieure

âœ… Publier rÃ©sultat **quel que soit le verdict** (positif = dÃ©couverte, nÃ©gatif = exclusion)

**DonnÃ©es** : Publiques, gratuites, disponibles maintenant

**DÃ©lai** : 1-2 heures (tÃ©lÃ©chargement + calcul)

**Impact** : Test dÃ©cisif de votre thÃ©orie

---

**PrÃªt Ã  lancer l'analyse ?** ðŸš€

Les scripts Python complets sont fournis ci-dessus. Dites-moi si vous voulez que je les crÃ©e en fichiers `.py` sÃ©parÃ©s !
